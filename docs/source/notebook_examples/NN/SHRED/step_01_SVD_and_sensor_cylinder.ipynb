{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHRED at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to use the [SHallow REcurrent Decoders](link) at scale. It consists on four different steps which entail the parallel reduction of the data and interpolation to the input sensors, the fit of the SHRED architecture, the evaluation and of the fitted SHRED configurations and finally the parallel reconstruction of the model using the POD coefficients predicted by SHRED.\n",
    "Although this tutorial can be launched all together in serial when working with such a small dataset, when working at scale each of the following steps is thought to be executed in separate scripts. Therefore, at introduction of each part of the tutorial, the link to the full script will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: parallel POD and sensor interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first step of the tutorial on how to use SHRED at scale. It consists on preparing large-scale data for the input (sensors) and output (POD coefficients) of the SHRED architecture. A script combining all chunks of code can be found [here](https://github.com/ArnauMiro/pyLowOrder/blob/120-add-shred/Examples/NN/SHRED/example_01_SVD_and_sensors_cylinder.py). This script can be executed with as many processors as needed because both the POD computation and the sensor interpolation are performed in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import mpi4py\n",
    "mpi4py.rc.recv_mprobe = False\n",
    "\n",
    "import numpy as np\n",
    "import pyLOM\n",
    "import matplotlib.pyplot as plt\n",
    "pyLOM.style_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the path to the pyLOM Dataset which we are going to work with and the list of dataset variables which we will use for this example. As you'll see later in the tutorial, both the sensor data and the POD coefficients have to be saved on dataset which is agnostic to the original partition (i.e. we need to use the nopartition=True), hence, we can only extract and save sensors of unidimensional variables and multidimensional fields will be omitted when saving in pyLOM format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = '/home/benet/Documents/repos/pyLowOrder/Examples/DATA/CYLINDER.h5'\n",
    "VARIABLE = 'VELOX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the mesh and dataset of the full-scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pyLOM.Mesh.load(DATAFILE)\n",
    "d = pyLOM.Dataset.load(DATAFILE,ptable=m.partition_table)\n",
    "t = d.get_variable('time')\n",
    "N = t.shape[0]\n",
    "pyLOM.pprint(0,'Number of available snapshots',N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split the dataset snapshots in training, validation and test. In this case the reconstruct mode splitting is used, therefore, the three different sets span the full data range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tridx, vaidx, teidx = pyLOM.math.data_splitting(N, mode='reconstruct')\n",
    "pyLOM.pprint(0,'Number of training snapshots %i, validation snapshots %i, test snapshots %i' % (tridx.shape[0], vaidx.shape[0], teidx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is defining the number of sensors and the region of the domain in which we want them to be. In this case only the X and Y axis are bounded because the input data is two-dimensional but in three-dimensional cases the Z axis bounds should also be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsens  = 3     \n",
    "x0, x1 = 0.5, 8 \n",
    "y0, y1 = -1, 1  \n",
    "bounds = np.array([x0,x1,y0,y1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate now the new pyLOM Dataset containing the sensor coordinates and the data values on them. In this case the sensors are an existing point of the full dataset which is randomly selected inside the bounds given by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsens  = d.select_random_sensors(nsens, bounds, [VARIABLE])\n",
    "print(dsens.xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensors time signals are now divided in train, validation and test following the data split computed earlier on this tutorial. A pyLOM Dataset is generated per split and saved to disk in nopartition mode so that it can be loaded serially when doing the SHRED fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrai = dsens.mask_fields(tridx,'time')\n",
    "dstrai.save('sensors_trai.h5', nopartition=True)\n",
    "dsvali = dsens.mask_fields(vaidx,'time')\n",
    "dsvali.save('sensors_vali.h5', nopartition=True)\n",
    "dstest = dsens.mask_fields(teidx,'time')\n",
    "dstest.save('sensors_test.h5', nopartition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the input is prepared, it's time to prepare the POD coefficients for the output. To do so, the POD modes are computed for the training test and then the validation and test snapshots are projected into it. Hence, we need to split the full dataset in train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrai = d.mask_fields(tridx,'time')\n",
    "dvali = d.mask_fields(vaidx,'time')\n",
    "dtest = d.mask_fields(teidx,'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute and save the POD modes of the training set. Note that the spatial modes and singular values are also saved as they will be needed to reconstruct the flow at the end of the tutorial. In this case we are computing 8 modes of the streamwise velocity fluctuations through randomized POD with 3 power iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrai   = dtrai[VARIABLE]\n",
    "PSI,S,V = pyLOM.POD.run(Xtrai,remove_mean=True,randomized=True,r=8,q=3)\n",
    "pyLOM.POD.save('POD_trai_%s.h5'%VARIABLE,dtrai.partition_table,U=PSI,S=S,V=V,nvars=1,pointData=dtrai.point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now project the validation and test snapshots to the training spatial modes. For the validation and test sets we only save their POD coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch validation dataset and project POD modes\n",
    "Xvali   = dvali[VARIABLE]\n",
    "proj    = pyLOM.math.matmulp(PSI.T, Xvali)\n",
    "Vvali   = pyLOM.math.matmul(pyLOM.math.diag(1/S), proj)\n",
    "pyLOM.POD.save('POD_vali_%s.h5'%VARIABLE,dvali.partition_table,V=Vvali,nvars=1,pointData=dvali.point)\n",
    "## Fetch test dataset and project POD modes\n",
    "Xtest   = dtest[VARIABLE]\n",
    "proj    = pyLOM.math.matmulp(PSI.T, Xtest)\n",
    "Vtest   = pyLOM.math.matmul(pyLOM.math.diag(1/S), proj)\n",
    "pyLOM.POD.save('POD_test_%s.h5'%VARIABLE,dtest.partition_table,V=Vtest,nvars=1,pointData=dtest.point)\n",
    "## Plot the first mode\n",
    "plt.figure()\n",
    "plt.plot(dtrai.get_variable('time'), V[0,:], 'rx', label='Training')\n",
    "plt.plot(dvali.get_variable('time'), Vvali[0,:], 'bo', label='Validation')\n",
    "plt.plot(dtest.get_variable('time'), Vtest[0,:], 'g*', label='Test')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'$V_1$')\n",
    "plt.title('First POD coefficient')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit SHRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been extracted and the POD has been computed, it is time to fit the SHRED model. This step is done in serial and the script combining all chunks of code can be found [here](https://github.com/ArnauMiro/pyLowOrder/blob/120-add-shred/Examples/NN/SHRED/example_02_fit_shred_cylinder.py). Note that this script ensambles different SHRED configurations for uncertainty quantification, but the core steps are the same as the ones in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pyLOM, pyLOM.NN\n",
    "\n",
    "class TimeSeriesDatasetMine(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Input: sequence of input measurements with shape (ntrajectories, ntimes, ninput) and corresponding measurements of high-dimensional state with shape (ntrajectories, ntimes, noutput)\n",
    "    Output: Torch dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X).permute(1,2,0)\n",
    "        self.Y = torch.tensor(Y).T\n",
    "        self.len = X.shape[1]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "device = pyLOM.NN.select_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define first the variable that is measured in the sensors and the one that we'll be reconstructing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensvar  = 'VELOX'    \n",
    "podvar   = 'VELOX' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the paths where we'll save the output of SHRED. Including the input scalers, the POD scaler and the weights of each SHRED configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inscaler = 'scalers_config_'\n",
    "outscale = 'scalers_pod.json'\n",
    "shreds   = 'shreds_config_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the sensor measurements that will be used for training, validation and test. Note that for each sensor we'll also load the snapshot ID (mask) and its time value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "data_trai = pyLOM.Dataset.load('sensors_trai.h5')\n",
    "sens_trai = data_trai[sensvar].astype(np.float32)\n",
    "nsens     = sens_trai.shape[0]\n",
    "mask_trai = data_trai.get_variable('mask')\n",
    "time_trai = data_trai.get_variable('time')\n",
    "# Validation\n",
    "data_vali = pyLOM.Dataset.load('sensors_vali.h5')\n",
    "sens_vali = data_vali[sensvar].astype(np.float32)\n",
    "mask_vali = data_vali.get_variable('mask')\n",
    "time_vali = data_vali.get_variable('time')\n",
    "# Test\n",
    "data_test = pyLOM.Dataset.load('sensors_test.h5')\n",
    "sens_test = data_test[sensvar].astype(np.float32)\n",
    "mask_test = data_test.get_variable('mask')\n",
    "time_test = data_test.get_variable('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full time vector can be reensambled for any plotting or reconstruction purposes in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntimeG = np.max(np.hstack((mask_trai,mask_vali,mask_test)))+1\n",
    "time   = np.zeros((ntimeG,), dtype=time_trai.dtype)\n",
    "time[mask_trai] = time_trai\n",
    "time[mask_vali] = time_vali\n",
    "time[mask_test] = time_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the POD coefficients of the training set together with their corresponding singular values. The singular values will be used to penalize the SHRED loss function so that the model puts more effort in learning the higher energy modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, pod_trai = pyLOM.POD.load('POD_trai_%s.h5' % podvar, vars=['S','V'])\n",
    "pod_trai    = pod_trai.astype(np.float32)\n",
    "output_size = pod_trai.shape[0]\n",
    "Sscale      = S/np.sum(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MinMaxScaler is created and fitted with the training POD data to scale the SHRED outputs between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_scaler  = pyLOM.NN.MinMaxScaler()\n",
    "pod_scaler.fit(pod_trai.T)\n",
    "pod_scaler.save(outscale)\n",
    "trai_out    = pod_scaler.transform(pod_trai.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the scaler is defined, the validation and test coefficients are loaded and transformed with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "pod_vali = pyLOM.POD.load('POD_vali_%s.h5' % podvar, vars='V')[0].astype(np.float32)\n",
    "vali_out = pod_scaler.transform(pod_vali.T).T\n",
    "# Test\n",
    "pod_test = pyLOM.POD.load('POD_test_%s.h5' % podvar, vars='V')[0].astype(np.float32)\n",
    "test_out = pod_scaler.transform(pod_test.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full POD coefficients can be reensambled for any plotting purposes in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pod = np.zeros((output_size,ntimeG), dtype=pod_trai.dtype)\n",
    "full_pod[:,mask_trai] = pod_trai\n",
    "full_pod[:,mask_vali] = pod_vali\n",
    "full_pod[:,mask_test] = pod_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the output size of the SHRED model (number of POD modes), we can generate the SHRED model. Note that the default hyperparameters are used in this case. A single sensor setup is used in this case, hence only a SHRED configuration will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shred     = pyLOM.NN.SHRED(output_size, device, nsens)\n",
    "mysensors = shred.configs[0]\n",
    "print(shred.configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the training, validation and test dataset of the sensors used in this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrai = sens_trai[mysensors,:]\n",
    "myvali = sens_vali[mysensors,:]\n",
    "mytest = sens_test[mysensors,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data of each sensor with a MinMaxScaler fitted on the training data. The MinMaxScaler is saved in a .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myscaler = pyLOM.NN.MinMaxScaler()\n",
    "scalpath = '%s%i.json' % (inscaler, 0)\n",
    "myscaler.fit(mytrai.T)\n",
    "myscaler.save(scalpath)\n",
    "trai_sca = myscaler.transform(mytrai.T).T\n",
    "vali_sca = myscaler.transform(myvali.T).T\n",
    "test_sca = myscaler.transform(mytest.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three different datasets are concatenated again to compute the time delay embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = np.zeros((trai_sca.shape[0],ntimeG), dtype=trai_sca.dtype)\n",
    "embedded[:,mask_trai] = trai_sca\n",
    "embedded[:,mask_vali] = vali_sca\n",
    "embedded[:,mask_test] = test_sca\n",
    "delayed = pyLOM.math.time_delay_embedding(embedded, dimension=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHRED model is fitted and its weights are saved together with the ID of the input sensors and the path to the used scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDatasetMine(delayed[:,mask_trai,:], trai_out) #TODO: use the pyLOM dataset or torch tensor dataset\n",
    "valid_dataset = TimeSeriesDatasetMine(delayed[:,mask_vali,:], vali_out) #TODO: use the pyLOM dataset\n",
    "shred.fit(train_dataset, valid_dataset, epochs=1500, patience=100, verbose=False, mod_scale=torch.tensor(Sscale))\n",
    "shred.save('%s%i' % (shreds,0), scalpath, outscale, mysensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: inference SHRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the SHRED models, it is time to evaluate their performance on reconstructing the POD coefficients. This step is also done in serial and reuses the results from the SHRED model fitted in the previous step. However, a script combining all chunks of code and working with N pre-trained SHRED configurations for uncertainty quantification can be found [here](https://github.com/ArnauMiro/pyLowOrder/blob/120-add-shred/Examples/NN/SHRED/example_03_inference_shred_cylinder.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first evaluate the SHRED Model with the full dataset (including training, validation and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = shred(torch.from_numpy(delayed).permute(1,2,0).to(device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we rescale the output using the MinMaxScaler used to scale the POD coefficients so that we can compare the SHRED output with the original values. The comparison is done using the Mean Relative Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outres = pod_scaler.inverse_transform(output).T\n",
    "MRE    = pyLOM.math.axiswise_r2(full_pod, outres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the predicted POD coefficients so that we can load them in the reconstruction step of the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLOM.POD.save('POD_predicted_%s.h5'% podvar,data_vali.partition_table,V=outres,nvars=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the SHRED performance on the POD coefficients with a bar plot that represents the MRE for each of the modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = pyLOM.utils.plotModalErrorBars(MRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by comparing the prediction of the time evolution of each of the modes with their original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = pyLOM.utils.plotTimeSeries(time, full_pod, outres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: parallel reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is loading the predicted POD coefficients and reconstructing the flow to evaluate its error regarding the original field. This part of the tutorial needs to be executed with the same amount of processors as Step 1 (where we extracted the data from the sensors and computed the POD). A script combining all chunks of code can be found [here](https://github.com/ArnauMiro/pyLowOrder/blob/120-add-shred/Examples/NN/SHRED/example_01_SVD_and_sensors_cylinder.py). Note that this script reloads the original dataset, the training POD modes and the predicted POD coefficients by SHRED, but in this case we can just reuse the computed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reconstructing the field from the training POD modes and singular values and then add the temporal mean of the original dataset (remember that until now we have been working with the fluctuations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_POD  = pyLOM.POD.reconstruct(PSI,S,outres)\n",
    "mean   = pyLOM.math.temporal_mean(d[VARIABLE])\n",
    "X_PODm = pyLOM.math.subtract_mean(X_POD, -1*mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the root mean square error between the original field and the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = pyLOM.math.RMSE(X_PODm,d[VARIABLE])\n",
    "pyLOM.pprint(0,'RMSE = %e'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add the reconstruction to the dataset so that we can plot the snapshots (serial cases) or save them to visualize them afterwards using any parallel visualization tool such as ParaView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.add_field('VELOR',1,X_PODm)\n",
    "pyLOM.POD.plotSnapshot(m,d.to_cpu(['VELOR']),vars=['VELOR'],instant=teidx[0],component=0,cmap='jet',cpos='xy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare it with the original flow at the same time instant (the first one in the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLOM.POD.plotSnapshot(m,d.to_cpu(['VELOX']),vars=['VELOX'],instant=teidx[0],component=0,cmap='jet',cpos='xy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
