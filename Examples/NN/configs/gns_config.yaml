# config.yaml â€” pyLOM experiment configuration

experiment:
  name: gns_nlr7301_baseline
  description: "Baseline GNS model on NLR7301 airfoil data"
  version: 1.0
  seed: 123
  device: cuda                      # Options: cpu | cuda | cuda:0 | etc.
  tags: [GNS, airfoil, baseline]
  resudir: "./results/exp_01"

datasets:
  train_ds: "./data/train_data.h5"
  val_ds: "./data/val_data.h5"
  test_ds: "./data/test_data.h5"

model:
  graph_path: "./data/mesh_graph.h5"
  input_dim: 2
  output_dim: 1
  latent_dim: 16
  hidden_size: 256
  num_msg_passing_layers: 3
  encoder_hidden_layers: 2
  decoder_hidden_layers: 1
  message_hidden_layers: 2
  update_hidden_layers: 2
  activation: ELU
  p_dropouts: 0.1

training:
  epochs: 500
  lr: 5e-4
  lr_gamma: 0.98
  lr_scheduler_step: 1
  optimizer: Adam
  scheduler: StepLR
  loss_fn: MSELoss
  batch_size: 16
  node_batch_size: 512
  num_workers: 4
  pin_memory: true
  verbose: 25

optuna:
  graph_path: "./data/mesh_graph.h5"
  study:
    study_name: gns_optuna_study
    direction: minimize
    timeout: 36000       # In seconds
    n_trials: 50
  search_space:
    model:
      latent_dim: [1, 64]
      hidden_size: [64, 512]
      num_msg_passing_layers: [1, 5]
      encoder_hidden_layers: [1, 8]
      decoder_hidden_layers: [1, 8]
      message_hidden_layers: [1, 8]
      update_hidden_layers: [1, 8]
      activation: [ELU, ReLU, LeakyReLU]
      p_dropouts: [0.0, 0.5]
    training:
      lr: [1e-5, 1e-3]
      lr_gamma: [0.9, 0.99, 0.999]
      lr_scheduler_step: [1, 10]
      batch_size: [8, 64]
      node_batch_size: [128, 597]
