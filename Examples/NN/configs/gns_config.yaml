# config.yaml â€” pyLOM experiment configuration

experiment:
  name: gns_nlr7301_baseline
  description: "GNS model on NLR7301 airfoil dataset as described by ..."
  version: 1.0
  tags: [GNS, airfoil, baseline]
  resudir: "../CETACEO_RESULTS/nlr7301/"
  mode: train                      # Options: train | optuna

datasets:
  train_ds: "../CETACEO_DATA/nlr7301/TRAIN_converter.h5"
  val_ds: "../CETACEO_DATA/nlr7301/VAL_converter.h5"
  test_ds: "../CETACEO_DATA/nlr7301/TEST_converter.h5"

model:
  graph_path: "../CETACEO_DATA/nlr7301/TRAIN_converter.h5"
  params:
    input_dim: 2
    output_dim: 1
    latent_dim: 16
    hidden_size: 256
    num_msg_passing_layers: 3
    encoder_hidden_layers: 2
    decoder_hidden_layers: 1
    message_hidden_layers: 2
    update_hidden_layers: 2
    groupnorm_groups: 2
    activation: ELU
    p_dropouts: 0.1
    seed: 42
    device: cuda

training:
  epochs: 3
  lr: 5e-4
  lr_gamma: 0.98
  lr_scheduler_step: 1
  optimizer: Adam
  scheduler: StepLR
  loss_fn: MSELoss
  batch_size: 16
  node_batch_size: 512
  num_workers: 4
  pin_memory: true
  verbose: 25

optuna:
  study:
    n_trials: 100
    direction: minimize
    pruner:
      type: MedianPruner
      n_startup_trials: 10
      n_warmup_steps: 5
    sampler:
      type: TPESampler
      multivariate: true
    seed: 42 # Used by sampler

  optimization_params:
    model:
      graph_path: "../CETACEO_DATA/nlr7301/TRAIN_converter.h5"
      params:
        input_dim: 2
        output_dim: 1
        latent_dim: {type: int, low: 8, high: 64}
        hidden_size: {type: int, low: 64, high: 512}
        num_msg_passing_layers: {type: int, low: 1, high: 5}
        encoder_hidden_layers: {type: int, low: 1, high: 4}
        decoder_hidden_layers: {type: int, low: 1, high: 4}
        message_hidden_layers: {type: int, low: 1, high: 4}
        update_hidden_layers: {type: int, low: 1, high: 4}
        groupnorms_groups: 2
        activation: {type: categorical, choices: [ELU, ReLU, LeakyReLU]}
        p_dropouts: {type: float, low: 0.0, high: 0.5}
        seed: 42
        device: cuda

    training:
      loss_fn: MSELoss
      optimizer: Adam
      scheduler: StepLR
      epochs: 3
      batch_size: {type: int, low: 8, high: 64}
      node_batch_size: {type: int, low: 128, high: 102
      lr: {type: float, low: 1e-5, high: 1e-3, log: true}
      lr_gamma: {type: float, low: 0.9, high: 0.999}
      lr_scheduler_step: {type: int, low: 1, high: 10}
      num_workers: 4
      pin_memory: true
      verbose: 25
