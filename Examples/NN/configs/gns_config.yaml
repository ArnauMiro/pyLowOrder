# config.yaml — pyLOM experiment configuration

experiment:
  name: gns_nlr7301_baseline
  description: "GNS model on NLR7301 airfoil dataset as described by ..."
  version: 1.0
  tags: [GNS, airfoil, baseline]
  results_path: "../CETACEO_RESULTS/nlr7301/"
  mode: optuna                      # Options: train | optuna

datasets:
  train_ds: "../CETACEO_DATA/nlr7301/TRAIN_converter.h5"
  val_ds: "../CETACEO_DATA/nlr7301/VAL_converter.h5"
  test_ds: "../CETACEO_DATA/nlr7301/TEST_converter.h5"

model:
  type: GNS
  graph_path: "../CETACEO_DATA/nlr7301/TRAIN_converter.h5"
  params:
    input_dim: 2
    output_dim: 1
    latent_dim: 16
    hidden_size: 256
    num_msg_passing_layers: 1
    encoder_hidden_layers: 6
    decoder_hidden_layers: 1
    message_hidden_layers: 2
    update_hidden_layers: 2
    groupnorm_groups: 2
    activation: torch.nn.ELU
    p_dropout: 0.0
    seed: 42
    device: cuda

training_type: default  # <-- ✅ Added to support dynamic dispatch

training:
  epochs: 1000
  lr: 6.5e-4
  lr_gamma: 0.9954
  lr_scheduler_step: 1
  optimizer: torch.optim.Adam
  scheduler: torch.optim.lr_scheduler.StepLR
  loss_fn: torch.nn.MSELoss
  print_every: 100

  dataloader:
    batch_size: 15
    shuffle: true
    num_workers: 4
    pin_memory: true

  subgraph_loader:
    batch_size: 32
    shuffle: true
    input_nodes: null  # Use default from dataloader


optuna:
  study:
    n_trials: 3
    direction: minimize
    pruner:
      type: optuna.pruners.MedianPruner
      n_startup_trials: 10
      n_warmup_steps: 5
    sampler:
      type: optuna.samplers.TPESampler
      multivariate: true
    seed: 42

  optimization_params:
    model:
      graph_path: "../CETACEO_DATA/nlr7301/TRAIN_converter.h5"
      params:
        input_dim: 2
        output_dim: 1
        latent_dim: {type: int, low: 8, high: 64, step: 2}
        hidden_size: {type: int, low: 64, high: 512}
        num_msg_passing_layers: {type: int, low: 1, high: 5}
        encoder_hidden_layers: {type: int, low: 1, high: 4}
        decoder_hidden_layers: {type: int, low: 1, high: 4}
        message_hidden_layers: {type: int, low: 1, high: 4}
        update_hidden_layers: {type: int, low: 1, high: 4}
        groupnorm_groups: 2
        activation: {type: categorical, choices: [torch.nn.ELU, torch.nn.ReLU, torch.nn.LeakyReLU]}
        p_dropout: {type: float, low: 0.0, high: 0.5}
        seed: 42
        device: cuda

    training:
      loss_fn: torch.nn.MSELoss
      optimizer: torch.optim.Adam
      scheduler: torch.optim.lr_scheduler.StepLR
      epochs: 3
      lr: {type: float, low: 1.0e-5, high: 1.0e-3, log: true}
      lr_gamma: {type: float, low: 0.9, high: 0.999}
      lr_scheduler_step: {type: int, low: 1, high: 10}
      print_every: 1

      dataloader:
        batch_size: {type: int, low: 8, high: 64}
        shuffle: true
        num_workers: 4
        pin_memory: true

      subgraph_loader:
        batch_size: {type: int, low: 1, high: 597}
        shuffle: true
        input_nodes: null
